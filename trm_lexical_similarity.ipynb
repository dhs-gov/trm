{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# TRM Lexical Similarity Tests\n",
    "This notebook tests *lexical* similarity between a scanned manufacturer name and a TRM manfacturer name. Since manufacturer names comprise single words or phrases and not sentences reflecting semantics or context, we do not perform *semantic* similarity using, for example, transformer-based models. Instead, we test some algorithms used specifically for computing lexical similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "The following utility functions are used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove company suffixes\n",
    "from cleanco import basename\n",
    "\n",
    "# Common technology company name suffixes (e.g., Systems, Software, Solutions, Techologies, etc.)\n",
    "suffix_stopwords = ['Software','Solutions','Systems','Technologies','Tech','Services','Communications',\n",
    "                    'Comms','Enterprises','Group','Networks','Associates','Assoc','Foundation','Organization','Org',\n",
    "                    'Project','Proj','Partners','Foundation','Company','Co','Corporation','Corp','Incorporated','Inc'\n",
    "                    ]\n",
    "\n",
    "def preproc(names):\n",
    "    preproc_list = []\n",
    "    for n in names:\n",
    "        # Remove common company suffixes (e.g., Corporation, Inc, etc.)\n",
    "        cleaned = basename(n)\n",
    "        # Remove common tech company suffixes (e.g., Software, Systems, etc.)\n",
    "        words = cleaned.split()\n",
    "        last_word = words[-1] # Get last word\n",
    "        if last_word.lower() in (string.lower() for string in suffix_stopwords):\n",
    "            cleaned = cleaned.rsplit(' ', 1)[0] # Remove last word\n",
    "        cleaned = cleaned.strip() # Remove preceding/trailing whitespace\n",
    "        preproc_list.append(cleaned)\n",
    "    return preproc_list\n",
    "\n",
    "# Test\n",
    "#x = preproc(['The Test Proj'])\n",
    "#print(f'Cleaned: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XLSX Results (not CSV, so we can later add formulas to XLSX)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "results_dir = 'C:\\\\work\\\\trm\\\\results'\n",
    "\n",
    "# Write results\n",
    "def write_csv(df, filename):\n",
    "    try:\n",
    "        # Set column headers\n",
    "        df.columns = [\"Scanned\", \"Predicted\", \"Score\", \"Expected\", \"Match\"]\n",
    "\n",
    "        now = datetime.now() # current date and time\n",
    "        date_time = now.strftime(\"%m%d%Y%H%M%S\")        \n",
    "        filepath = Path(results_dir + '\\\\' + filename + '_' + date_time + '.xlsx')  \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "        with pd.ExcelWriter(filepath) as writer:\n",
    "            df.to_excel(writer)  \n",
    "        #df.to_csv(filepath) \n",
    "        print(f'Generated results at: {filepath}')\n",
    "    except Exception as e:\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Stats\n",
    "def get_stats(results):\n",
    "    num_0_matches = 0  # Num errors\n",
    "    num_1_matches = 0  # Num correct\n",
    "    num_2_matches = 0  # Num unknown (assess manually)\n",
    "    for x in results:\n",
    "        #print(f'scan={x[0]}, best={x[1]}, score={x[2]}, expected={x[3]}, match={x[4]}')\n",
    "        if x[4] == 0:\n",
    "            num_0_matches = num_0_matches + 1\n",
    "        if x[4] == 1:\n",
    "            num_1_matches = num_1_matches + 1\n",
    "        elif x[4] == 2:\n",
    "            num_2_matches = num_2_matches + 1\n",
    "\n",
    "    total = len(results)\n",
    "    perc_0 = num_0_matches / total\n",
    "    perc_1 = num_1_matches / total\n",
    "    perc_2 = num_2_matches / total\n",
    "    print(f'Correct: {perc_1}%, Incorrect: {perc_0}%, Unknown: {perc_2}%')\n",
    "    print(f'Final scores must be assessed manually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Input: Official TRM Data\n",
    "Get the manufacturer names and IDs from the official TRM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRM rows (dedup): 3674\n"
     ]
    }
   ],
   "source": [
    "# Read official TRM XLSX into a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "# Read official TRM data\n",
    "df_trm = pd.read_excel('C:\\\\work\\\\trm\\\\ramya_files\\\\TRM_official.xlsx', sheet_name='1 - component-baseline')\n",
    "\n",
    "# Remove rows with duplicate manufacturer name\n",
    "df_trm_dedup = df_trm.drop_duplicates(subset=['Manufacturer Name'])\n",
    "\n",
    "# Sort row by ascending manufacturer names\n",
    "df_trm_dedup_sorted = df_trm_dedup.sort_values(by=['Manufacturer Name'])\n",
    "\n",
    "# Get manufacturer names\n",
    "trm_mfr_names = df_trm_dedup_sorted['Manufacturer Name']\n",
    "# Convert to list to allow for indexing\n",
    "trm_mfr_list = list(trm_mfr_names)\n",
    "# Get preprocessed mfr names\n",
    "trm_preproc_mfr = preproc(trm_mfr_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: Scanned Data\n",
    "Get the input data (i.e., manufacturer data scanned from the network) and the previously predicted results. We will use previously predicted results as the target we are expected to predict. *NOTE: The input file should be converted to an XLSX file and deduplicated on the scanned manufacturer column before ingesting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scanned manufacturer data\n",
    "df_scan = pd.read_excel('C:\\\\work\\\\trm\\\\ramya_files\\\\Manufacturer_LRpredictions_2_dedup.xlsx')\n",
    "\n",
    "# Get scanned manufacturer names\n",
    "scan_mfr_names = df_scan['manufacturer']\n",
    "# Convert to list to allow for indexing\n",
    "scan_mfr_names_list = list(scan_mfr_names)\n",
    "\n",
    "# Get preprocessed manufacturer names:\n",
    "scan_preproc_mfr = preproc(scan_mfr_names)\n",
    "\n",
    "# Get previously predicted results that we will use as our expected results\n",
    "expected_mfr_names = df_scan['predict_manufacturer']\n",
    "# Convert to list to allow for indexing\n",
    "expected_mfr_names_list = list(expected_mfr_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexical Similarity Analyses\n",
    "### Config\n",
    "Configuration parametes for all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure tests\n",
    "max_rows = None  # If no max, set to None\n",
    "\n",
    "# Break loops on (perfect) score == 1.0 or dist == 0.0\n",
    "break_on_perfect_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "Note that SpaCy Doc.similarity requires target words to be 'included' in its internal dictionary (with the possibly of some minor spelling errors). Otherwise, it will not be able to generate word vectors for comparison and result with a similarity score of 0.0. Regardless, we test the performanc of SpaCy for comparison purposes.\n",
    "\n",
    "WARNING: This SpaCy code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# SpaCy en_core_web_lg contains word vectors -- en_core_web_sm does not.\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc.similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "w1 = nlp('Microsft')\n",
    "w2 = nlp('Microsoft')\n",
    "sim_score = w1.similarity(w2)\n",
    "sim2_score = w2.similarity(w1)\n",
    "print(f'score: {sim2_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing scan word 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SQuirol\\AppData\\Local\\Temp\\1\\ipykernel_6140\\456244400.py:13: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim_score = scan_mfr_tokens.similarity(trm_mfr_tokens)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated results at: C:\\work\\trm\\results\\spacy_similarity_02192022170858.xlsx\n",
      "Correct: 0.5957446808510638%, Incorrect: 0.031914893617021274%, Unknown: 0.3723404255319149%\n",
      "Final scores must be assessed manually.\n"
     ]
    }
   ],
   "source": [
    "# TRM\n",
    "results = []\n",
    "for i, scan_preproc in enumerate(scan_preproc_mfr):\n",
    "    print(f'Analyzing scan word {i}', end='\\r')\n",
    "    best_sim_score = 0.0\n",
    "    scan_index = -1\n",
    "    best_mfr_name = None\n",
    "    for j, trm_preproc in enumerate(trm_preproc_mfr):\n",
    "        \n",
    "        # Similarity\n",
    "        scan_mfr_tokens = nlp(scan_preproc)\n",
    "        trm_mfr_tokens = nlp(trm_preproc)\n",
    "        sim_score = scan_mfr_tokens.similarity(trm_mfr_tokens)\n",
    "        \n",
    "        if sim_score > best_sim_score:\n",
    "            best_sim_score = sim_score\n",
    "            best_mfr_name = trm_mfr_list[j]\n",
    "            scan_index = i\n",
    "            if break_on_perfect_score and best_sim_score == 1.0:\n",
    "                break\n",
    "            \n",
    "    matches_expected = 0\n",
    "    if best_sim_score == 1.0:\n",
    "        matches_expected = 1\n",
    "    elif pd.isna(expected_mfr_names_list[scan_index]):\n",
    "        matches_expected = 2\n",
    "    elif best_mfr_name == expected_mfr_names_list[scan_index]:\n",
    "        matches_expected = 1\n",
    "        \n",
    "    results.append([scan_mfr_names_list[i], best_mfr_name, best_sim_score, expected_mfr_names_list[scan_index], matches_expected])\n",
    "    \n",
    "    if max_rows != None and i == max_rows:\n",
    "        break\n",
    "    \n",
    "df_results = pd.DataFrame.from_records(results)\n",
    "write_csv(df_results, 'spacy_similarity')\n",
    "\n",
    "get_stats(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levenshtein edit-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist: 7\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "dist = nltk.edit_distance('asdfasdf', 'Microsoft')\n",
    "print(f'dist: {dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated results at: C:\\work\\trm\\results\\nltk_levenshtein_02192022162225.xlsx\n",
      "Correct: 0.6276595744680851%, Incorrect: 0.010638297872340425%, Unknown: 0.3617021276595745%\n",
      "Final scores must be assessed manually.\n"
     ]
    }
   ],
   "source": [
    "# TRM\n",
    "results = []\n",
    "for i, scan_preproc in enumerate(scan_preproc_mfr):\n",
    "    print(f'Analyzing scan word {i}', end='\\r')\n",
    "    best_sim_score = 100.0\n",
    "    scan_index = -1\n",
    "    best_mfr_name = None\n",
    "    for j, trm_preproc in enumerate(trm_preproc_mfr):\n",
    "        \n",
    "        # Similarity\n",
    "        if len(scan_preproc.split()) < len(trm_preproc.split()):\n",
    "            trm_name_array = trm_preproc.split()[:len(scan_preproc.split())]\n",
    "            trm_name = ' '.join(trm_name_array)\n",
    "        else:\n",
    "            trm_name = trm_preproc\n",
    "            \n",
    "        dist = nltk.edit_distance(scan_preproc, trm_name)\n",
    "        \n",
    "        if dist < best_sim_score:\n",
    "            best_sim_score = dist\n",
    "            best_mfr_name = trm_mfr_list[j]\n",
    "            scan_index = i\n",
    "            if break_on_perfect_score and best_sim_score == 0.0:\n",
    "                break\n",
    "            \n",
    "    matches_expected = 0\n",
    "    if best_sim_score == 0.0:\n",
    "        matches_expected = 1\n",
    "    elif pd.isna(expected_mfr_names_list[scan_index]):\n",
    "        matches_expected = 2\n",
    "    elif best_mfr_name == expected_mfr_names_list[scan_index]:\n",
    "        matches_expected = 1\n",
    "        \n",
    "    results.append([scan_mfr_names_list[i], best_mfr_name, best_sim_score, expected_mfr_names_list[scan_index], matches_expected])\n",
    "    \n",
    "    if max_rows != None and i == max_rows:\n",
    "        break\n",
    "    \n",
    "df_results = pd.DataFrame.from_records(results)\n",
    "write_csv(df_results, 'nltk_levenshtein')\n",
    "\n",
    "get_stats(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "dist = nltk.jaccard_distance(set('asdfasdf'), set('Microsoft'))\n",
    "print(f'dist: {dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated results at: C:\\work\\trm\\results\\nltk_jaccard_02192022175112.xlsx\n",
      "Correct: 0.6382978723404256%, Incorrect: 0.010638297872340425%, Unknown: 0.35106382978723405%\n",
      "Final scores must be assessed manually.\n"
     ]
    }
   ],
   "source": [
    "# TRM\n",
    "results = []\n",
    "for i, scan_preproc in enumerate(scan_preproc_mfr):\n",
    "    print(f'Analyzing scan word {i}', end='\\r')\n",
    "    best_sim_score = 100.0\n",
    "    scan_index = -1\n",
    "    best_mfr_name = None\n",
    "    for j, trm_preproc in enumerate(trm_preproc_mfr):\n",
    "        \n",
    "        # Similarity\n",
    "        if len(scan_preproc.split()) < len(trm_preproc.split()):\n",
    "            trm_name_array = trm_preproc.split()[:len(scan_preproc.split())]\n",
    "            trm_name = ' '.join(trm_name_array)\n",
    "        else:\n",
    "            trm_name = trm_preproc\n",
    "            \n",
    "        dist = nltk.jaccard_distance(set(scan_preproc), set(trm_name))\n",
    "\n",
    "        if dist < best_sim_score:\n",
    "            best_sim_score = dist\n",
    "            best_mfr_name = trm_mfr_list[j]\n",
    "            scan_index = i\n",
    "            if break_on_perfect_score and best_sim_score == 0.0:\n",
    "                break\n",
    "            \n",
    "    matches_expected = 0\n",
    "    if best_sim_score == 0.0:\n",
    "        matches_expected = 1\n",
    "    elif pd.isna(expected_mfr_names_list[scan_index]):\n",
    "        matches_expected = 2\n",
    "    elif best_mfr_name == expected_mfr_names_list[scan_index]:\n",
    "        matches_expected = 1\n",
    "        \n",
    "    results.append([scan_mfr_names_list[i], best_mfr_name, best_sim_score, expected_mfr_names_list[scan_index], matches_expected])\n",
    "    \n",
    "    if max_rows != None and i == max_rows:\n",
    "        break\n",
    "    \n",
    "df_results = pd.DataFrame.from_records(results)\n",
    "write_csv(df_results, 'nltk_jaccard')\n",
    "\n",
    "get_stats(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def word2vec(word):\n",
    "    from collections import Counter\n",
    "    from math import sqrt\n",
    "\n",
    "    # count the characters in word\n",
    "    cw = Counter(word)\n",
    "    # precomputes a set of the different characters\n",
    "    sw = set(cw)\n",
    "    # precomputes the \"length\" of the word vector\n",
    "    lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "    # return a tuple\n",
    "    return cw, sw, lw\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "v1 = word2vec('Cisco')\n",
    "v2 = word2vec('Cisco')\n",
    "sim = cosdis(v1, v2)\n",
    "print(f'sim: {sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated results at: C:\\work\\trm\\results\\word2vec_cosine_02192022162424.xlsx\n",
      "Correct: 0.6276595744680851%, Incorrect: 0.010638297872340425%, Unknown: 0.3617021276595745%\n",
      "Final scores must be assessed manually.\n"
     ]
    }
   ],
   "source": [
    "# TRM\n",
    "results = []\n",
    "for i, scan_preproc in enumerate(scan_preproc_mfr):\n",
    "    print(f'Analyzing scan word {i}', end='\\r')\n",
    "    best_sim_score = 0.0\n",
    "    scan_index = -1\n",
    "    best_mfr_name = None\n",
    "    for j, trm_preproc in enumerate(trm_preproc_mfr):\n",
    "        \n",
    "        # Similarity\n",
    "        if len(scan_preproc.split()) < len(trm_preproc.split()):\n",
    "            trm_name_array = trm_preproc.split()[:len(scan_preproc.split())]\n",
    "            trm_name = ' '.join(trm_name_array)\n",
    "        else:\n",
    "            trm_name = trm_preproc\n",
    "            \n",
    "        scan_vec = word2vec(scan_preproc)\n",
    "        trm_vec = word2vec(trm_name)\n",
    "        sim = cosdis(scan_vec, trm_vec)\n",
    "        \n",
    "        if sim > best_sim_score:\n",
    "            best_sim_score = sim\n",
    "            best_mfr_name = trm_mfr_list[j]\n",
    "            scan_index = i\n",
    "            if break_on_perfect_score and best_sim_score == 1.0:\n",
    "                break\n",
    "            \n",
    "    matches_expected = 0\n",
    "    if best_sim_score >= 1:  # For some reason, some best scores are 1.0000000000000002\n",
    "        matches_expected = 1    \n",
    "    elif pd.isna(expected_mfr_names_list[scan_index]):\n",
    "        matches_expected = 2\n",
    "    elif best_mfr_name == expected_mfr_names_list[scan_index]:\n",
    "        matches_expected = 1\n",
    "        \n",
    "    results.append([scan_mfr_names_list[i], best_mfr_name, best_sim_score, expected_mfr_names_list[scan_index], matches_expected])\n",
    "    \n",
    "    if max_rows != None and i == max_rows:\n",
    "        break\n",
    "    \n",
    "df_results = pd.DataFrame.from_records(results)\n",
    "write_csv(df_results, 'word2vec_cosine')\n",
    "\n",
    "get_stats(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft, 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sm = difflib.SequenceMatcher(None)\n",
    "\n",
    "sm.set_seq2('Microsoft')\n",
    "test = 'Microsoft'\n",
    "sm.set_seq1(test)\n",
    "print(f' {test}, {sm.ratio()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated results at: C:\\work\\trm\\results\\difflib_dist_02192022180116.xlsx\n",
      "Correct: 0.6276595744680851%, Incorrect: 0.010638297872340425%, Unknown: 0.3617021276595745%\n",
      "Final scores must be assessed manually.\n"
     ]
    }
   ],
   "source": [
    "# TRM\n",
    "results = []\n",
    "for i, scan_preproc in enumerate(scan_preproc_mfr):\n",
    "    print(f'Analyzing scan word {i}', end='\\r')\n",
    "    best_sim_score = 0.0\n",
    "    scan_index = -1\n",
    "    best_mfr_name = None\n",
    "    \n",
    "    sm = difflib.SequenceMatcher(None)\n",
    "    sm.set_seq2(scan_preproc)\n",
    "\n",
    "    for j, trm_preproc in enumerate(trm_preproc_mfr):\n",
    "        \n",
    "        # Similarity\n",
    "        sm.set_seq1(trm_preproc)\n",
    "        sim = sm.ratio()\n",
    "        \n",
    "        if sim > best_sim_score:\n",
    "            best_sim_score = sim\n",
    "            best_mfr_name = trm_mfr_list[j]\n",
    "            scan_index = i\n",
    "            #print(f'scan: {scan_preproc}, trm: {best_mfr_name}, best_score: {best_sim_score}')\n",
    "            if break_on_perfect_score and best_sim_score == 1.0:\n",
    "                break\n",
    "            \n",
    "    matches_expected = 0\n",
    "    if best_sim_score >= 1:  # For some reason, some best scores are 1.0000000000000002\n",
    "        matches_expected = 1    \n",
    "    elif pd.isna(expected_mfr_names_list[scan_index]):\n",
    "        matches_expected = 2\n",
    "    elif best_mfr_name == expected_mfr_names_list[scan_index]:\n",
    "        matches_expected = 1\n",
    "        \n",
    "    results.append([scan_mfr_names_list[i], best_mfr_name, best_sim_score, expected_mfr_names_list[scan_index], matches_expected])\n",
    "    \n",
    "    if max_rows != None and i == max_rows:\n",
    "        break\n",
    "    \n",
    "df_results = pd.DataFrame.from_records(results)\n",
    "write_csv(df_results, 'difflib_dist')\n",
    "\n",
    "get_stats(results)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "a65e8ab6709122d8eeedb731a245b9ba92924b4391628943345bea28822fe229"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
